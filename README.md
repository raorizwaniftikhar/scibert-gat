# ğŸ§  Hierarchical Multi-Label Classification with Graph and Contextual Embeddings

This repository implements a multi-stage hierarchical classification framework that combines semantic and structural information through SciBERT and Graph Attention Networks (GAT). The architecture is designed to improve classification accuracy across multi-level category hierarchies.

---

## ğŸ“Œ Overview

The model consists of **five main stages**:

1. **Data Preparation**
2. **Contextual and Graph Embedding Creation**
3. **Feature Fusion via Gated Mechanisms**
4. **Dual Encoding and Classification**
5. **Evaluation with Hierarchical Metrics**

![Architecture Diagram](Proposed-Methodology.png)

---

## 1. ğŸ“‚ Data Preparation

We preprocess the input text (titles and abstracts) before passing them through SciBERT.

### ğŸ”§ Steps:

* **Text Cleaning**: Lowercasing, removing special characters, digits-only tokens, and excessive whitespaces.
* **SciBERT Tokenization**: Tokenization using WordPiece via the `BERTTokenizer` from SciBERT.

This ensures meaningful, model-friendly token inputs for downstream embedding.

---

## 2. ğŸ”— Graph Construction

Instead of word-level graphs, we build a **category-level graph** representing label hierarchies.

### ğŸŒ Category Graph:

* **Nodes**: Labels from three hierarchical levels (e.g., `cat1 â†’ cat2 â†’ cat3`)
* **Edges**: Directed edges encode parent-child relationships.

### ğŸ§  Graph Embeddings:

A **Graph Attention Network (GAT)** is applied to learn structural label embeddings that reflect inter-label dependencies.

---

## 3. ğŸ§¬ Graph Attention Network (GAT)

GAT models hierarchical label relationships through attention-based message passing.

### ğŸ“‹ Key Steps:

* **Node Initialization**: Each label is initialized with a trainable vector.
* **Attention Mechanism**: Determines the relevance of neighboring nodes.
* **Message Passing**: Aggregates information from neighbors based on learned attention weights.
* **Stacking Layers**: Deepens understanding of graph structure across layers.

These embeddings capture **semantic and structural dependencies**, enriching classification.

---

## 4. âœ¨ SciBERT Contextual Embeddings

SciBERT is used to generate **contextual embeddings** from preprocessed titles and abstracts.

### ğŸ“Œ Workflow:

* Input: Preprocessed text
* Output: Context Vectors (CVs) capturing deep semantic meaning

---

## 5. ğŸ” Feature Fusion

We integrate contextual and structural embeddings through a **gated fusion mechanism**.

### ğŸ§ª Fusion Components:

* **CV**: Context Vector from SciBERT
* **GV**: Graph Vector from GAT
* **Gated Fusion**: A gating layer learns to scale and combine both vectors optimally.

This enhances the representational power of input features for downstream classification.

---

## 6. ğŸ—ï¸ Encoding Layer

The fused features undergo **dual encoding**:

* **Transformer Encoder**: Captures intra-document relationships (sentence-level).
* **Hierarchical Attention Network (HAN)**: Extracts document-level semantics by attending to important sentence features.

---

## 7. ğŸ§® Classification

* Final output from HAN is passed through a **fully connected layer** with **sigmoid activation** for multi-label classification across all 3 levels.
* Ensures **hierarchical coherence**â€”child categories are only predicted if their parent category is active.

---

## 8. ğŸ“ Evaluation Metrics

We use both **standard and hierarchical** evaluation metrics.

### ğŸ“ Hierarchical Metrics:

Let `Táµ¢` be the set of true labels (including ancestors) and `Páµ¢` the predicted labels for the *i-th* sample.

* **Hierarchical Precision**
  [
  HP = \frac{1}{N} \sum_{i=1}^{N} \frac{|Páµ¢ \cap Táµ¢|}{|Páµ¢|}
  ]

* **Hierarchical Recall**
  [
  HR = \frac{1}{N} \sum_{i=1}^{N} \frac{|Páµ¢ \cap Táµ¢|}{|Táµ¢|}
  ]

* **Hierarchical F1 Score**
  [
  HF1 = \frac{2 \cdot HP \cdot HR}{HP + HR}
  ]

### âœ… Other Metrics:

* **Subset Accuracy**:
  [
  SA = \frac{1}{N} \sum_{i=1}^{N} 1(Páµ¢ = Táµ¢)
  ]

* **Hamming Loss**:
  [
  HL = \frac{1}{N \cdot L} \sum_{i=1}^{N} \sum_{j=1}^{L} 1(y_{i,j} \ne \hat{y}_{i,j})
  ]

---

## 9. ğŸ§ª Testing

During inference, the same pipeline is followed:

* Preprocessing
* Embedding via SciBERT
* GAT-based structural learning
* Feature fusion
* Encoding and classification

All trained parameters are used without gradient updates.

---

## ğŸ“ Folder Structure

```
â”œâ”€â”€ data/                # Raw and processed data
â”œâ”€â”€ models/              # Saved model weights
â”œâ”€â”€ src/                 # Source code
â”‚   â”œâ”€â”€ final-model.ipynb
â”œâ”€â”€ images/
â”‚   â””â”€â”€ Proposed-Methodology.png
â”œâ”€â”€ README.md
```
